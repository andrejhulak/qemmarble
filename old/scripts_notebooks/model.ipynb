{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class SequenceModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, model_type, p_dropout):\n",
    "        super(SequenceModel, self).__init__()\n",
    "        self.qubit_models = nn.ModuleDict()\n",
    "        # this is always gonne be 5 for now on (based on FakeLima :) )\n",
    "        if model_type == 'LSTM':\n",
    "            for i in range(5):\n",
    "                self.qubit_models[f'sequence_{i}'] = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=p_dropout)\n",
    "        elif model_type == 'RNN':\n",
    "            for i in range(5):\n",
    "                self.qubit_models[f'sequence_{i}'] = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=p_dropout)\n",
    "        elif model_type == 'Transformer':\n",
    "            print('To be added soon, initialising with LSTM for now!')\n",
    "            for i in range(5):\n",
    "                self.qubit_models[f'sequence_{i}'] = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=p_dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        models_output = []\n",
    "        for i, model in enumerate(self.qubit_models.values()):\n",
    "            model_output, _ = model(x[i])\n",
    "            model_output_last = model_output[-1, :]   \n",
    "            models_output.append(model_output_last) \n",
    "        \n",
    "        output = torch.stack(models_output, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ANN(nn.Module):\n",
    "  def __init__(self,\n",
    "               input_shape: int,\n",
    "               hidden_units: int,\n",
    "               hidden_layers: int,\n",
    "               output_shape: int,\n",
    "               p_drouput: int):\n",
    "    super().__init__()\n",
    "    self.layers = nn.ModuleDict()\n",
    "    self.nLayers = hidden_layers\n",
    "\n",
    "    self.layers['input'] = nn.Linear(input_shape,\n",
    "                                     hidden_units)\n",
    "\n",
    "    for i in range(hidden_layers):\n",
    "      self.layers[f'hidden{i}'] = nn.Linear(hidden_units,\n",
    "                                            hidden_units)\n",
    "\n",
    "    self.layers['output'] = nn.Linear(hidden_units,\n",
    "                                      output_shape)\n",
    "    \n",
    "    self.dropout = p_drouput\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.flatten(x)\n",
    "    ReLU = nn.ReLU()\n",
    "\n",
    "    x = ReLU(self.layers['input'](x))\n",
    "\n",
    "    for i in range(self.nLayers):\n",
    "        x = ReLU(self.layers[f'hidden{i}'](x))\n",
    "        x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "    x = self.layers['output'](x)\n",
    "\n",
    "    return x\n",
    "  \n",
    "    \n",
    "def create_models(sequence_input_size: int,\n",
    "                  sequence_hidden_size: int,\n",
    "                  sequence_num_layers: int,\n",
    "                  sequence_model_type: str,\n",
    "                  sequence_dropout: int,\n",
    "                  ann_hiden_layers: int, \n",
    "                  ann_hidden_units: int,\n",
    "                  ann_dropout: int,\n",
    "                  n_qubits = 5):\n",
    "\n",
    "    sequence_model = SequenceModel(input_size=sequence_input_size, hidden_size=sequence_hidden_size, num_layers=sequence_num_layers, model_type=sequence_model_type, p_dropout=sequence_dropout)\n",
    "    ann = ANN(input_shape=(sequence_hidden_size * n_qubits + 1), hidden_units=ann_hidden_units, hidden_layers=ann_hiden_layers, output_shape=1, p_drouput=ann_dropout)\n",
    "\n",
    "    return sequence_model, ann\n",
    "\n",
    "def run_models(sequence_model, ann, x, noisy_exp_val):\n",
    "    # go through rnns\n",
    "    sequence_model_output = sequence_model(x)\n",
    "\n",
    "    # flatten for ann\n",
    "    sequence_model_output = torch.flatten(sequence_model_output)\n",
    "\n",
    "    # convert noisy exp val to appropriate dtype\n",
    "    noisy_exp_val = torch.tensor(np.float32(noisy_exp_val))\n",
    "\n",
    "    # add noisy exp val to sequence_model output\n",
    "    sequence_model_output_noisy_exp_val = torch.cat((sequence_model_output, noisy_exp_val.reshape(1)))\n",
    "\n",
    "    # go through ann\n",
    "    model_output = ann(sequence_model_output_noisy_exp_val)\n",
    "\n",
    "    return model_output\n",
    "\n",
    "def train_step(sequence_model, ann, loss_fn, X, noisy_exp_vals, y, optimiser):\n",
    "    sequence_model.train()\n",
    "    ann.train()\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        y_pred = run_models(sequence_model, ann, X[i], noisy_exp_vals[i])\n",
    "\n",
    "        loss = loss_fn(y_pred, y[i].unsqueeze(dim=0))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "    \n",
    "    return train_loss/len(X)\n",
    "\n",
    "def test_step(sequence_model, ann, loss_fn, X, noisy_exp_vals, y):\n",
    "    sequence_model.eval()\n",
    "    ann.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for i in range(len(X)):\n",
    "            y_pred = run_models(sequence_model, ann, X[i], noisy_exp_vals[i])\n",
    "\n",
    "            loss = loss_fn(y_pred, y[i].unsqueeze(dim=0))\n",
    "\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "        return test_loss/len(X)\n",
    "\n",
    "def train_and_test_step(sequence_model, ann, loss_fn, optimiser, X_train, train_noisy_exp_vals, y_train, X_test, test_noisy_exp_vals, y_test, num_epochs, print_results=True):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "        train_loss = train_step(sequence_model=sequence_model,\n",
    "                                ann=ann,\n",
    "                                loss_fn=loss_fn,\n",
    "                                X=X_train,\n",
    "                                noisy_exp_vals=train_noisy_exp_vals,\n",
    "                                y=y_train,\n",
    "                                optimiser=optimiser)\n",
    "        \n",
    "        test_loss = test_step(sequence_model=sequence_model,\n",
    "                              ann=ann,\n",
    "                              loss_fn=loss_fn,\n",
    "                              X=X_test,\n",
    "                              noisy_exp_vals=test_noisy_exp_vals,\n",
    "                              y=y_test)\n",
    "\n",
    "        if epoch % 1 == 0 & print_results:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, train loss: {np.sqrt(train_loss):.4f}, test_loss: {np.sqrt(test_loss):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
